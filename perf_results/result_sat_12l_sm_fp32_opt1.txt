*** Measurement Settings ***
  Batch size: 1
  Service Kind: TRITON
  Using "time_windows" mode for stabilization
  Stabilizing using average latency and throughput
  Measurement window: 5000 msec
  Latency limit: 0 msec
  Concurrency limit: 4 concurrent requests
  Using synchronous calls for inference

Request concurrency: 1
  Client: 
    Request count: 1201
    Throughput: 66.6928 infer/sec
    Avg latency: 14978 usec (standard deviation 14629 usec)
    p50 latency: 14556 usec
    p90 latency: 14824 usec
    p95 latency: 14927 usec
    p99 latency: 15218 usec
    Avg HTTP time: 14969 usec (send/recv 60 usec + response wait 14909 usec)
  Server: 
    Inference count: 1201
    Execution count: 1201
    Successful request count: 1201
    Avg request latency: 14485 usec (overhead 13 usec + queue 21 usec + compute input 39 usec + compute infer 14401 usec + compute output 10 usec)

Request concurrency: 2
  Client: 
    Request count: 1258
    Throughput: 69.868 infer/sec
    Avg latency: 28598 usec (standard deviation 556 usec)
    p50 latency: 28644 usec
    p90 latency: 28932 usec
    p95 latency: 29013 usec
    p99 latency: 29183 usec
    Avg HTTP time: 28584 usec (send/recv 80 usec + response wait 28504 usec)
  Server: 
    Inference count: 1258
    Execution count: 1258
    Successful request count: 1258
    Avg request latency: 28047 usec (overhead 18 usec + queue 13747 usec + compute input 21 usec + compute infer 14244 usec + compute output 16 usec)

Request concurrency: 3
  Client: 
    Request count: 1244
    Throughput: 69.0864 infer/sec
    Avg latency: 43400 usec (standard deviation 795 usec)
    p50 latency: 43435 usec
    p90 latency: 43976 usec
    p95 latency: 44093 usec
    p99 latency: 44397 usec
    Avg HTTP time: 43386 usec (send/recv 85 usec + response wait 43301 usec)
  Server: 
    Inference count: 1244
    Execution count: 1244
    Successful request count: 1244
    Avg request latency: 42824 usec (overhead 18 usec + queue 28356 usec + compute input 20 usec + compute infer 14413 usec + compute output 16 usec)

Request concurrency: 4
  Client: 
    Request count: 1230
    Throughput: 68.3094 infer/sec
    Avg latency: 58518 usec (standard deviation 863 usec)
    p50 latency: 58539 usec
    p90 latency: 59188 usec
    p95 latency: 59405 usec
    p99 latency: 59670 usec
    Avg HTTP time: 58504 usec (send/recv 86 usec + response wait 58418 usec)
  Server: 
    Inference count: 1230
    Execution count: 1230
    Successful request count: 1230
    Avg request latency: 57899 usec (overhead 17 usec + queue 43269 usec + compute input 18 usec + compute infer 14579 usec + compute output 14 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 66.6928 infer/sec, latency 14978 usec
Concurrency: 2, throughput: 69.868 infer/sec, latency 28598 usec
Concurrency: 3, throughput: 69.0864 infer/sec, latency 43400 usec
Concurrency: 4, throughput: 68.3094 infer/sec, latency 58518 usec
