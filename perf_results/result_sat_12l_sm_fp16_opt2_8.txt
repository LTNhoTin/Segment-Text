*** Measurement Settings ***
  Batch size: 1
  Service Kind: TRITON
  Using "time_windows" mode for stabilization
  Stabilizing using average latency and throughput
  Measurement window: 5000 msec
  Latency limit: 0 msec
  Concurrency limit: 8 concurrent requests
  Using synchronous calls for inference

Request concurrency: 1
  Client: 
    Request count: 1783
    Throughput: 98.9952 infer/sec
    Avg latency: 10087 usec (standard deviation 263 usec)
    p50 latency: 10085 usec
    p90 latency: 10416 usec
    p95 latency: 10504 usec
    p99 latency: 10740 usec
    Avg HTTP time: 10077 usec (send/recv 65 usec + response wait 10012 usec)
  Server: 
    Inference count: 1784
    Execution count: 1784
    Successful request count: 1784
    Avg request latency: 9634 usec (overhead 17 usec + queue 20 usec + compute input 27 usec + compute infer 9558 usec + compute output 11 usec)

Request concurrency: 2
  Client: 
    Request count: 1850
    Throughput: 102.733 infer/sec
    Avg latency: 19442 usec (standard deviation 373 usec)
    p50 latency: 19418 usec
    p90 latency: 19863 usec
    p95 latency: 20010 usec
    p99 latency: 20256 usec
    Avg HTTP time: 19430 usec (send/recv 85 usec + response wait 19345 usec)
  Server: 
    Inference count: 1851
    Execution count: 1851
    Successful request count: 1851
    Avg request latency: 18829 usec (overhead 17 usec + queue 9110 usec + compute input 20 usec + compute infer 9668 usec + compute output 13 usec)

Request concurrency: 3
  Client: 
    Request count: 1846
    Throughput: 102.522 infer/sec
    Avg latency: 29245 usec (standard deviation 523 usec)
    p50 latency: 29200 usec
    p90 latency: 29841 usec
    p95 latency: 29972 usec
    p99 latency: 30285 usec
    Avg HTTP time: 29233 usec (send/recv 82 usec + response wait 29151 usec)
  Server: 
    Inference count: 1846
    Execution count: 1846
    Successful request count: 1846
    Avg request latency: 28631 usec (overhead 14 usec + queue 18884 usec + compute input 17 usec + compute infer 9704 usec + compute output 11 usec)

Request concurrency: 4
  Client: 
    Request count: 1848
    Throughput: 102.627 infer/sec
    Avg latency: 38936 usec (standard deviation 587 usec)
    p50 latency: 38893 usec
    p90 latency: 39538 usec
    p95 latency: 39772 usec
    p99 latency: 40175 usec
    Avg HTTP time: 38924 usec (send/recv 82 usec + response wait 38842 usec)
  Server: 
    Inference count: 1848
    Execution count: 1848
    Successful request count: 1848
    Avg request latency: 38344 usec (overhead 17 usec + queue 28613 usec + compute input 18 usec + compute infer 9682 usec + compute output 13 usec)

Request concurrency: 5
  Client: 
    Request count: 1842
    Throughput: 102.296 infer/sec
    Avg latency: 48839 usec (standard deviation 586 usec)
    p50 latency: 48839 usec
    p90 latency: 49324 usec
    p95 latency: 49479 usec
    p99 latency: 50039 usec
    Avg HTTP time: 48827 usec (send/recv 80 usec + response wait 48747 usec)
  Server: 
    Inference count: 1842
    Execution count: 1842
    Successful request count: 1842
    Avg request latency: 48273 usec (overhead 16 usec + queue 38509 usec + compute input 18 usec + compute infer 9718 usec + compute output 12 usec)

Request concurrency: 6
  Client: 
    Request count: 1841
    Throughput: 102.24 infer/sec
    Avg latency: 58636 usec (standard deviation 713 usec)
    p50 latency: 58625 usec
    p90 latency: 59299 usec
    p95 latency: 59482 usec
    p99 latency: 59916 usec
    Avg HTTP time: 58624 usec (send/recv 80 usec + response wait 58544 usec)
  Server: 
    Inference count: 1842
    Execution count: 1842
    Successful request count: 1842
    Avg request latency: 58045 usec (overhead 17 usec + queue 48276 usec + compute input 18 usec + compute infer 9721 usec + compute output 12 usec)

Request concurrency: 7
  Client: 
    Request count: 1845
    Throughput: 102.463 infer/sec
    Avg latency: 68282 usec (standard deviation 708 usec)
    p50 latency: 68270 usec
    p90 latency: 68838 usec
    p95 latency: 69094 usec
    p99 latency: 69672 usec
    Avg HTTP time: 68270 usec (send/recv 80 usec + response wait 68190 usec)
  Server: 
    Inference count: 1845
    Execution count: 1845
    Successful request count: 1845
    Avg request latency: 67670 usec (overhead 15 usec + queue 57918 usec + compute input 17 usec + compute infer 9708 usec + compute output 11 usec)

Request concurrency: 8
  Client: 
    Request count: 1840
    Throughput: 102.185 infer/sec
    Avg latency: 78222 usec (standard deviation 772 usec)
    p50 latency: 78210 usec
    p90 latency: 78963 usec
    p95 latency: 79202 usec
    p99 latency: 79533 usec
    Avg HTTP time: 78209 usec (send/recv 82 usec + response wait 78127 usec)
  Server: 
    Inference count: 1840
    Execution count: 1840
    Successful request count: 1840
    Avg request latency: 77629 usec (overhead 17 usec + queue 67856 usec + compute input 19 usec + compute infer 9724 usec + compute output 13 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 98.9952 infer/sec, latency 10087 usec
Concurrency: 2, throughput: 102.733 infer/sec, latency 19442 usec
Concurrency: 3, throughput: 102.522 infer/sec, latency 29245 usec
Concurrency: 4, throughput: 102.627 infer/sec, latency 38936 usec
Concurrency: 5, throughput: 102.296 infer/sec, latency 48839 usec
Concurrency: 6, throughput: 102.24 infer/sec, latency 58636 usec
Concurrency: 7, throughput: 102.463 infer/sec, latency 68282 usec
Concurrency: 8, throughput: 102.185 infer/sec, latency 78222 usec
